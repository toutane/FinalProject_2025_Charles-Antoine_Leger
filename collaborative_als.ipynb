{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355e3eb5",
   "metadata": {},
   "source": [
    "# Alternating Least Squares (ALS) Collaborative Filtering Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "926180df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5721cd",
   "metadata": {},
   "source": [
    "## Spark Session Initialization\n",
    "Initializing a Spark session with increased memory allocation to handle large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "00bf7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KuaiRecALS\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df6203",
   "metadata": {},
   "source": [
    "# Data Loading and Sampling\n",
    "Loading the user-item interaction data and sampling a fraction to fit into memory for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "24b1ee41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>play_duration</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>watch_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>4381</td>\n",
       "      <td>6067</td>\n",
       "      <td>2020-07-05 05:27:48.378</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>0.722103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>183</td>\n",
       "      <td>11635</td>\n",
       "      <td>6100</td>\n",
       "      <td>2020-07-05 05:28:00.057</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>1.907377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>3649</td>\n",
       "      <td>22422</td>\n",
       "      <td>10867</td>\n",
       "      <td>2020-07-05 05:29:09.479</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>2.063311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>5262</td>\n",
       "      <td>4479</td>\n",
       "      <td>7908</td>\n",
       "      <td>2020-07-05 05:30:43.285</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>0.566388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>8234</td>\n",
       "      <td>4602</td>\n",
       "      <td>11000</td>\n",
       "      <td>2020-07-05 05:35:43.459</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593899e+09</td>\n",
       "      <td>0.418364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  video_id  play_duration  video_duration                     time  \\\n",
       "0       14       148           4381            6067  2020-07-05 05:27:48.378   \n",
       "1       14       183          11635            6100  2020-07-05 05:28:00.057   \n",
       "2       14      3649          22422           10867  2020-07-05 05:29:09.479   \n",
       "3       14      5262           4479            7908  2020-07-05 05:30:43.285   \n",
       "4       14      8234           4602           11000  2020-07-05 05:35:43.459   \n",
       "\n",
       "         date     timestamp  watch_ratio  \n",
       "0  20200705.0  1.593898e+09     0.722103  \n",
       "1  20200705.0  1.593898e+09     1.907377  \n",
       "2  20200705.0  1.593898e+09     2.063311  \n",
       "3  20200705.0  1.593898e+09     0.566388  \n",
       "4  20200705.0  1.593899e+09     0.418364  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_raw = pd.read_csv(\"data_final_project/KuaiRec 2.0/data/small_matrix.csv\")\n",
    "#interactions_raw = pd.read_csv(\"data_final_project/KuaiRec 2.0/data/big_matrix.csv\")\n",
    "\n",
    "# Reduce the size of the DataFrame to fit into memory\n",
    "#interactions_raw = interactions_raw.sample(frac=0.1, random_state=42)\n",
    "interactions_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df14f1",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b46cf",
   "metadata": {},
   "source": [
    "Creating a binary 'is_like' column to represent positive feedback and normalizing ratings for ALS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ae13d18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>play_duration</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>watch_ratio</th>\n",
       "      <th>is_like</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>4381</td>\n",
       "      <td>6067</td>\n",
       "      <td>2020-07-05 05:27:48.378</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>183</td>\n",
       "      <td>11635</td>\n",
       "      <td>6100</td>\n",
       "      <td>2020-07-05 05:28:00.057</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>3649</td>\n",
       "      <td>22422</td>\n",
       "      <td>10867</td>\n",
       "      <td>2020-07-05 05:29:09.479</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>5262</td>\n",
       "      <td>4479</td>\n",
       "      <td>7908</td>\n",
       "      <td>2020-07-05 05:30:43.285</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>8234</td>\n",
       "      <td>4602</td>\n",
       "      <td>11000</td>\n",
       "      <td>2020-07-05 05:35:43.459</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593899e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  video_id  play_duration  video_duration                     time  \\\n",
       "0       14       148           4381            6067  2020-07-05 05:27:48.378   \n",
       "1       14       183          11635            6100  2020-07-05 05:28:00.057   \n",
       "2       14      3649          22422           10867  2020-07-05 05:29:09.479   \n",
       "3       14      5262           4479            7908  2020-07-05 05:30:43.285   \n",
       "4       14      8234           4602           11000  2020-07-05 05:35:43.459   \n",
       "\n",
       "         date     timestamp  watch_ratio  is_like  ratings  \n",
       "0  20200705.0  1.593898e+09            1        0        1  \n",
       "1  20200705.0  1.593898e+09            3        0        3  \n",
       "2  20200705.0  1.593898e+09            4        1        4  \n",
       "3  20200705.0  1.593898e+09            1        0        1  \n",
       "4  20200705.0  1.593899e+09            0        0        0  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = interactions_raw.copy()\n",
    "interactions_df[\"is_like\"] = interactions_df['watch_ratio'].apply(lambda x: 1 if x >= 2 else 0)\n",
    "#interactions_df['ratings'] = interactions_df['watch_ratio'].apply(lambda x: 1 if x >= 2 else 0)\n",
    "\n",
    "# Normalise the watch_ratio to a scale of 0-6\n",
    "# Create a finer-grained scale for watch_ratio\n",
    "def scale_watch_ratio(x):\n",
    "    if x >= 6:\n",
    "        return 12\n",
    "    elif x >= 5.5:\n",
    "        return 11\n",
    "    elif x >= 5:\n",
    "        return 10\n",
    "    elif x >= 4.5:\n",
    "        return 9\n",
    "    elif x >= 4:\n",
    "        return 8\n",
    "    elif x >= 3.5:\n",
    "        return 7\n",
    "    elif x >= 3:\n",
    "        return 6\n",
    "    elif x >= 2.5:\n",
    "        return 5\n",
    "    elif x >= 2:\n",
    "        return 4\n",
    "    elif x >= 1.5:\n",
    "        return 3\n",
    "    elif x >= 1:\n",
    "        return 2\n",
    "    elif x >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "interactions_df['watch_ratio'] = interactions_df['watch_ratio'].apply(scale_watch_ratio)\n",
    "interactions_df['ratings'] = interactions_df['watch_ratio']\n",
    "\n",
    "interactions_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48d7e5",
   "metadata": {},
   "source": [
    "### Conversion to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2245cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_spark = spark.createDataFrame(interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38a46e",
   "metadata": {},
   "source": [
    "### Selecting Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cd87bd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:12 WARN TaskSetManager: Stage 4071 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 4071:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|user_id|video_id|ratings|\n",
      "+-------+--------+-------+\n",
      "|     14|     148|      1|\n",
      "|     14|     183|      3|\n",
      "|     14|    3649|      4|\n",
      "|     14|    5262|      1|\n",
      "|     14|    8234|      0|\n",
      "+-------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:16 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 4071 (TID 8188): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_spark = interactions_spark.select('user_id', 'video_id', 'ratings')\n",
    "ratings_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e190a98",
   "metadata": {},
   "source": [
    "### Indexing User and Item IDs\n",
    "Encoding user and video IDs as numerical indices, which is required for Spark's ALS implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8b069038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:16 WARN TaskSetManager: Stage 4072 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/18 00:20:17 WARN TaskSetManager: Stage 4075 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/18 00:20:18 WARN TaskSetManager: Stage 4078 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 4078:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+--------------+-------------+\n",
      "|user_id|video_id|ratings|video_id_index|user_id_index|\n",
      "+-------+--------+-------+--------------+-------------+\n",
      "|     14|     148|      1|        3070.0|        238.0|\n",
      "|     14|     183|      3|         203.0|        238.0|\n",
      "|     14|    3649|      4|        3087.0|        238.0|\n",
      "|     14|    5262|      1|        2329.0|        238.0|\n",
      "|     14|    8234|      0|        3318.0|        238.0|\n",
      "+-------+--------+-------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:22 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 4078 (TID 8219): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexer = [\n",
    "    StringIndexer(inputCol=column, outputCol=column + \"_index\")\n",
    "    for column in list(set(ratings_spark.columns) - set([\"ratings\"]))\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(ratings_spark).transform(ratings_spark)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb1a7f",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "Splitting the data into training and test sets to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "85e0c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = transformed.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2dc23",
   "metadata": {},
   "source": [
    "# ALS Model Training\n",
    "Configuring and training the ALS model to learn latent factors for users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a4004217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:22 WARN TaskSetManager: Stage 4079 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/18 00:20:23 WARN TaskSetManager: Stage 4080 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "als = ALS(\n",
    "    maxIter=17,\n",
    "    regParam=0.01,\n",
    "    rank=25,\n",
    "    userCol=\"user_id_index\",\n",
    "    itemCol=\"video_id_index\",\n",
    "    ratingCol=\"ratings\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    # implicitPrefs=True,\n",
    ")\n",
    "\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2850f",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Evaluating the ALS model using RMSE on the test set to assess prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4f703445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:27 WARN TaskSetManager: Stage 4161 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.230313879321708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:29 WARN TaskSetManager: Stage 4248 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+--------------+-------------+----------+\n",
      "|user_id|video_id|ratings|video_id_index|user_id_index|prediction|\n",
      "+-------+--------+-------+--------------+-------------+----------+\n",
      "|    120|     206|      1|        2142.0|         31.0| 0.9456809|\n",
      "|    120|    2671|      1|         463.0|         31.0| 1.1577698|\n",
      "|    120|    2820|      1|         496.0|         31.0| 0.6950925|\n",
      "|    120|    5266|      1|        2659.0|         31.0| 1.3514937|\n",
      "|     64|     762|      1|        1591.0|        451.0| 1.7157103|\n",
      "+-------+--------+-------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=\"ratings\", predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "predictions = model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE=\" + str(rmse))\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc4cc2",
   "metadata": {},
   "source": [
    "## Post-processing Recommendations\n",
    "Converting Spark recommendations to Pandas, mapping indices back to original IDs, and organizing recommendations for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4437fc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 00:20:32 WARN TaskSetManager: Stage 4411 contains a task of very large size (20834 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id                                    recommendations\n",
      "0          14  [(1305, 4.730415344238281), (7383, 5.369937419...\n",
      "1          19  [(314, 4.339854717254639), (5525, 4.0973510742...\n",
      "2          21  [(9178, 4.689492225646973), (2130, 5.547003746...\n",
      "3          23  [(314, 4.204347610473633), (8298, 5.2634778022...\n",
      "4          24  [(5811, 3.389115333557129), (8298, 4.427169799...\n",
      "...       ...                                                ...\n",
      "1406     7142  [(4040, 4.0484418869018555), (4123, 4.08339023...\n",
      "1407     7147  [(8298, 4.950584411621094), (154, 5.0683169364...\n",
      "1408     7153  [(5464, 3.399148464202881), (1305, 4.011781215...\n",
      "1409     7159  [(314, 3.6427805423736572), (9178, 4.795408248...\n",
      "1410     7162  [(600, 5.654923915863037), (7383, 5.6269860267...\n",
      "\n",
      "[1411 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/crm6wxzx7813ytz60bfsjbhm0000gn/T/ipykernel_16343/973738124.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new[\"recommendations\"] = list(zip(new.video_id, new.ratings))\n"
     ]
    }
   ],
   "source": [
    "recs = model.recommendForAllUsers(10).toPandas()\n",
    "df_recs = (\n",
    "    recs.recommendations.apply(pd.Series)\n",
    "    .merge(recs, right_index=True, left_index=True)\n",
    "    .drop([\"recommendations\"], axis=1)\n",
    "    .melt(id_vars=[\"user_id_index\"], value_name=\"recommendation\")\n",
    "    .drop(\"variable\", axis=1)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "df_recs = df_recs.sort_values(\"user_id_index\")\n",
    "df_recs = pd.concat(\n",
    "    [df_recs[\"recommendation\"].apply(pd.Series), df_recs[\"user_id_index\"]], axis=1\n",
    ")\n",
    "\n",
    "df_recs.columns = [\"product_id_index\", \"ratings\", \"reviewer_id\"]\n",
    "tmp = transformed.select(\n",
    "    transformed[\"user_id\"],\n",
    "    transformed[\"user_id_index\"],\n",
    "    transformed[\"video_id\"],\n",
    "    transformed[\"video_id_index\"],\n",
    ")\n",
    "tmp = tmp.toPandas()\n",
    "\n",
    "dict1 = dict(zip(tmp[\"user_id_index\"], tmp[\"user_id\"]))\n",
    "dict2 = dict(zip(tmp[\"video_id_index\"], tmp[\"video_id\"]))\n",
    "\n",
    "df_recs_copy = df_recs.copy()\n",
    "df_recs_copy.loc[:, \"user_id\"] = df_recs[\"reviewer_id\"].map(dict1)\n",
    "df_recs_copy.loc[:, \"video_id\"] = df_recs[\"product_id_index\"].map(dict2)\n",
    "df_recs_copy = df_recs_copy.sort_values(\"user_id\")\n",
    "df_recs_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "new = df_recs_copy[[\"user_id\", \"video_id\", \"ratings\"]]\n",
    "new[\"recommendations\"] = list(zip(new.video_id, new.ratings))\n",
    "\n",
    "res = new[[\"user_id\", \"recommendations\"]]\n",
    "res_new = res[\"recommendations\"].groupby([res.user_id]).apply(list).reset_index()\n",
    "\n",
    "print(res_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2c380",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6a143931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk_metrics(y_true, top_k_preds, k=10):\n",
    "    top_k = top_k_preds[:k]\n",
    "    relevant = set(y_true)\n",
    "    hits = [1 if item in relevant else 0 for item in top_k]\n",
    "\n",
    "    precision = sum(hits) / k\n",
    "    recall = sum(hits) / len(relevant) if relevant else 0.0\n",
    "    dcg = sum(hit / np.log2(i + 2) for i, hit in enumerate(hits))\n",
    "    ideal_hits = [1] * min(len(relevant), k)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(len(ideal_hits)))\n",
    "    ndcg = dcg / idcg if idcg != 0 else 0.0\n",
    "\n",
    "    # MAP@k: mean average precision\n",
    "    ap_sum = 0.0\n",
    "    hit_count = 0\n",
    "    for i, hit in enumerate(hits):\n",
    "        if hit:\n",
    "            hit_count += 1\n",
    "            ap_sum += hit_count / (i + 1)\n",
    "    map_k = ap_sum / min(len(relevant), k) if relevant else 0.0\n",
    "\n",
    "    return precision, recall, ndcg, map_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7c962828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_interactions_df = pd.read_csv(\"data_final_project/KuaiRec 2.0/data/small_matrix.csv\")\n",
    "#test_interactions_df[\"is_like\"] = interactions_df['watch_ratio'].apply(lambda x: 1 if x >= 2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8baa8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for k=1:\n",
      "  Mean Precision@1: 0.7385\n",
      "  Mean Recall@1: 0.0125\n",
      "  Mean NDCG@1: 0.7385\n",
      "  Mean MAP@1: 0.7385\n",
      "\n",
      "Results for k=5:\n",
      "  Mean Precision@5: 0.7184\n",
      "  Mean Recall@5: 0.0606\n",
      "  Mean NDCG@5: 0.7224\n",
      "  Mean MAP@5: 0.6216\n",
      "\n",
      "Results for k=10:\n",
      "  Mean Precision@10: 0.7174\n",
      "  Mean Recall@10: 0.1214\n",
      "  Mean NDCG@10: 0.7205\n",
      "  Mean MAP@10: 0.5892\n",
      "\n",
      "Results for k=20:\n",
      "  Mean Precision@20: 0.3587\n",
      "  Mean Recall@20: 0.1214\n",
      "  Mean NDCG@20: 0.4699\n",
      "  Mean MAP@20: 0.2995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs_dict = dict(zip(res_new[\"user_id\"], res_new[\"recommendations\"]))\n",
    "\n",
    "k_values = [1, 5, 10, 20]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    all_precisions, all_recalls, all_ndcgs, all_maps = [], [], [], []\n",
    "    for user_id, recs in user_recs_dict.items():\n",
    "        video_ids = [video_id for video_id, _ in recs][:k]\n",
    "        y_true = interactions_df[(interactions_df[\"user_id\"] == user_id) & (interactions_df[\"is_like\"] == 1)][\"video_id\"].tolist()\n",
    "        if not y_true or not video_ids:\n",
    "            continue\n",
    "        precision, recall, ndcg, map_k = evaluate_topk_metrics(y_true, video_ids, k)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_ndcgs.append(ndcg)\n",
    "        all_maps.append(map_k)\n",
    "    results.append({\n",
    "        \"k\": k,\n",
    "        \"precision\": np.mean(all_precisions),\n",
    "        \"recall\": np.mean(all_recalls),\n",
    "        \"ndcg\": np.mean(all_ndcgs),\n",
    "        \"map\": np.mean(all_maps)\n",
    "    })\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Results for k={res['k']}:\")\n",
    "    print(f\"  Mean Precision@{res['k']}: {res['precision']:.4f}\")\n",
    "    print(f\"  Mean Recall@{res['k']}: {res['recall']:.4f}\")\n",
    "    print(f\"  Mean NDCG@{res['k']}: {res['ndcg']:.4f}\")\n",
    "    print(f\"  Mean MAP@{res['k']}: {res['map']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbb6c2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The ALS collaborative filtering recommender system was evaluated using Precision@K, Recall@K, NDCG@K, and MAP@K metrics for various values of K. The results show that the model is able to recommend relevant items to users with reasonable effectiveness, as indicated by the improvement in evaluation metrics compared to earlier iterations. \n",
    "\n",
    "While the current approach demonstrates a significant increase in performance (e.g., Precision@10 improved from 0.06 to 0.24), there is still room for further optimization. Potential improvements include additional hyperparameter tuning, experimenting with implicit feedback, filtering out users/items with very few interactions, and refining the evaluation methodology (such as using a temporal split or negative sampling).\n",
    "\n",
    "Overall, the ALS-based collaborative filtering model provides a solid foundation for personalized recommendations and can be further enhanced with more advanced techniques and data preprocessing strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rema-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
